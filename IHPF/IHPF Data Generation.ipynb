{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read original datasets\n",
    "\n",
    "Code adapted from Scanorama \n",
    "\n",
    "https://github.com/brianhie/scanorama "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os.path\n",
    "import scipy.sparse\n",
    "from scipy.sparse import csr_matrix, csc_matrix, coo_matrix\n",
    "from sklearn.preprocessing import normalize\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "MIN_TRANSCRIPTS = 600\n",
    "\n",
    "def load_tab(fname, max_genes=40000):\n",
    "    if fname.endswith('.gz'):\n",
    "        opener = gzip.open\n",
    "    else:\n",
    "        opener = open\n",
    "        \n",
    "    with opener(fname, 'r') as f:\n",
    "        if fname.endswith('.gz'):\n",
    "            header = f.readline().decode('utf-8').rstrip().split('\\t')\n",
    "        else:\n",
    "            header = f.readline().rstrip().split('\\t')\n",
    "            \n",
    "        cells = header[1:]\n",
    "        X = np.zeros((len(cells), max_genes))\n",
    "        genes = []\n",
    "        for i, line in enumerate(f):\n",
    "            if i > max_genes:\n",
    "                break\n",
    "            if fname.endswith('.gz'):\n",
    "                line = line.decode('utf-8')\n",
    "            fields = line.rstrip().split('\\t')\n",
    "            genes.append(fields[0])\n",
    "            X[:, i] = [ float(f) for f in fields[1:] ]\n",
    "    return X[:, range(len(genes))], np.array(cells), np.array(genes)\n",
    "\n",
    "def load_mtx(dname):\n",
    "    with open(dname + '/matrix.mtx', 'r') as f:\n",
    "        while True:\n",
    "            header = f.readline()\n",
    "            if not header.startswith('%'):\n",
    "                break\n",
    "        header = header.rstrip().split()\n",
    "        n_genes, n_cells = int(header[0]), int(header[1])\n",
    "\n",
    "        data, i, j = [], [], []\n",
    "        for line in f:\n",
    "            fields = line.rstrip().split()\n",
    "            data.append(float(fields[2]))\n",
    "            i.append(int(fields[1])-1)\n",
    "            j.append(int(fields[0])-1)\n",
    "        X = csr_matrix((data, (i, j)), shape=(n_cells, n_genes))\n",
    "\n",
    "    genes = []\n",
    "    with open(dname + '/genes.tsv', 'r') as f:\n",
    "        for line in f:\n",
    "            fields = line.rstrip().split()\n",
    "            genes.append(fields[1])\n",
    "    assert(len(genes) == n_genes)\n",
    "\n",
    "    return X, np.array(genes)\n",
    "\n",
    "def process_tab(fname, min_trans=MIN_TRANSCRIPTS):\n",
    "    X, cells, genes = load_tab(fname)\n",
    "\n",
    "    gt_idx = [ i for i, s in enumerate(np.sum(X != 0, axis=1))\n",
    "               if s >= min_trans ]\n",
    "    X = X[gt_idx, :]\n",
    "    cells = cells[gt_idx]\n",
    "    if len(gt_idx) == 0:\n",
    "        print('Warning: 0 cells passed QC in {}'.format(fname))\n",
    "    if fname.endswith('.txt'):\n",
    "        cache_prefix = '.'.join(fname.split('.')[:-1])\n",
    "    elif fname.endswith('.txt.gz'):\n",
    "        cache_prefix = '.'.join(fname.split('.')[:-2])\n",
    "    elif fname.endswith('.tsv'):\n",
    "        cache_prefix = '.'.join(fname.split('.')[:-1])\n",
    "    elif fname.endswith('.tsv.gz'):\n",
    "        cache_prefix = '.'.join(fname.split('.')[:-2])\n",
    "    else:\n",
    "        sys.stderr.write('Tab files should end with \".txt\" or \".tsv\"\\n')\n",
    "        exit(1)\n",
    "        \n",
    "    cache_fname = cache_prefix + '.npz'\n",
    "    #np.savez(cache_fname, X=X, genes=genes)\n",
    "\n",
    "    return X, cells, genes\n",
    "\n",
    "def process_mtx(dname, min_trans=MIN_TRANSCRIPTS):\n",
    "    X, genes = load_mtx(dname)\n",
    "\n",
    "    gt_idx = [ i for i, s in enumerate(np.sum(X != 0, axis=1))\n",
    "               if s >= min_trans ]\n",
    "    X = X[gt_idx, :]\n",
    "    if len(gt_idx) == 0:\n",
    "        print('Warning: 0 cells passed QC in {}'.format(dname))\n",
    "    \n",
    "    cache_fname = dname + '/tab.npz'\n",
    "    #scipy.sparse.save_npz(cache_fname, X, compressed=False)\n",
    "\n",
    "    #with open(dname + '/tab.genes.txt', 'w') as of:\n",
    "        #of.write('\\n'.join(genes) + '\\n')\n",
    "\n",
    "    return X, genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_real_data(counter):\n",
    "\n",
    "    # Assume real datasets have cell as rows and genes as columns No index \n",
    "    # keep track of real data mappings \n",
    "    datamapping = { 1:'pancreas_inDrop', 2:'pancreas_multi_celseq2_expression_matrix',\n",
    "                    4:'pancreas_multi_fluidigmc1_expression_matrix', 5:'pancreas_multi_smartseq2_expression_matrix',\n",
    "                    3:'pancreas_multi_celseq_expression_matrix',\n",
    "                    7:'jurkat', 6:'293t', 8:'jurkat_293t_50_50', 9:'jurkat_293t_99_1',\n",
    "                    10:'68k_pbmc', 11:'b_cells', 12:'cd14_monocytes', 13:'cd4_t_helper', 14:'cd56_nk',\n",
    "                    15:'cytotoxic_t', 16:'memory_t', 17:'regulatory_t', 18:'pbmc_10X', 19:'pbmc_kang',\n",
    "                    20:'nuclei', 21:'Cerebellum_ALT', 22:'Cortex_noRep5_FRONTALonly', 23:'Cortex_noRep5_POSTERIORonly',\n",
    "                    24:'EntoPeduncular', 25:'GlobusPallidus', 26:'Hippocampus', 27:'Striatum',\n",
    "                    28:'SubstantiaNigra', 29:'Thalamus', 30:'GSM3589406_PP001swap.filtered.matrix',\n",
    "                    31:'GSM3589407_PP002swap.filtered.matrix', 32:'GSM3589408_PP003swap.filtered.matrix',\n",
    "                    33:'GSM3589409_PP004swap.filtered.matrix'}\n",
    "    \n",
    "    # \n",
    "    datasetname = datamapping[counter]\n",
    "\n",
    "    ## TODO: Map to a sequence of related datasets in sparse matrix format\n",
    "\n",
    "    if counter in [1,2,3,4,5]:\n",
    "        genecountdata, cell_array, gene_array = process_tab('../Real/scanorama/pancreas/{}.txt.gz'.format(datasetname))\n",
    "        genecountdata = csr_matrix(genecountdata)\n",
    "        print('Read real dataset {}'.format(datasetname))    \n",
    "\n",
    "    if counter in [6,7,8,9]:\n",
    "        genecountdata, gene_array = process_mtx('../Real/scanorama/293t_jurkat/{}'.format(datasetname))\n",
    "        genecountdata = csr_matrix(genecountdata)\n",
    "        print('Read real dataset {}'.format(datasetname))   \n",
    "\n",
    "    if counter in [10,11,12,13,14,15,16,17]:\n",
    "        genecountdata, gene_array = process_mtx('../Real/scanorama/pbmc/10x/{}'.format(datasetname))\n",
    "        genecountdata = csr_matrix(genecountdata)\n",
    "        print('Read real dataset {}'.format(datasetname))   \n",
    "\n",
    "    if counter in [18,19]:\n",
    "        genecountdata, cell_array, gene_array = process_tab('../Real/scanorama/pbmc/{}.txt.gz'.format(datasetname))\n",
    "        genecountdata = csr_matrix(genecountdata)\n",
    "        print('Read real dataset {}'.format(datasetname))\n",
    "\n",
    "    if counter in [20]:\n",
    "        genecountdata, cell_array, gene_array = process_mtx('../Real/scanorama/mouse_brain/{}'.format(datasetname))\n",
    "        genecountdata = csr_matrix(genecountdata)\n",
    "        print('Read real dataset {}'.format(datasetname))\n",
    "\n",
    "    if counter in [21,22,23,24,25,26,27,28,29]:\n",
    "        genecountdata, cell_array, gene_array = process_mtx('../Real/scanorama/mouse_brain/dropviz/{}'.format(datasetname))\n",
    "        genecountdata = csr_matrix(genecountdata)\n",
    "        print('Read real dataset {}'.format(datasetname))\n",
    "\n",
    "    if counter in [30,31,32,33,34]:\n",
    "        genecountdata, cell_array, gene_array = process_tab('../Real/Levitin_bloodT/{}.txt.gz'.format(datasetname))\n",
    "        genecountdata = csr_matrix(genecountdata)\n",
    "        print('Read real dataset {}'.format(datasetname))\n",
    "\n",
    "\n",
    "    return datasetname, genecountdata, gene_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put datasets into a single matrix with the intersection of all genes.\n",
    "def merge_datasets(datasets, genes, ds_names=None, verbose=True, union=False):\n",
    "    if union:\n",
    "        sys.stderr.write(\n",
    "            \"WARNING: Integrating based on the union of genes is \"\n",
    "            \"highly discouraged, consider taking the intersection \"\n",
    "            \"or requantifying gene expression.\\n\"\n",
    "        )\n",
    "\n",
    "    # Find genes in common.\n",
    "    keep_genes = set()\n",
    "    for idx, gene_list in enumerate(genes):\n",
    "        if len(keep_genes) == 0:\n",
    "            keep_genes = set(gene_list)\n",
    "        elif union:\n",
    "            keep_genes |= set(gene_list)\n",
    "        else:\n",
    "            keep_genes &= set(gene_list)\n",
    "        if not union and not ds_names is None and verbose:\n",
    "            print(\"After {}: {} genes\".format(ds_names[idx], len(keep_genes)))\n",
    "        if len(keep_genes) == 0:\n",
    "            print(\"Error: No genes found in all datasets, exiting...\")\n",
    "            exit(1)\n",
    "    if verbose:\n",
    "        print(\"Found {} genes among all datasets\".format(len(keep_genes)))\n",
    "\n",
    "    if union:\n",
    "        union_genes = sorted(keep_genes)\n",
    "        for i in range(len(datasets)):\n",
    "            if verbose:\n",
    "                print(\"Processing data set {}\".format(i))\n",
    "            X_new = np.zeros((datasets[i].shape[0], len(union_genes)))\n",
    "            X_old = csc_matrix(datasets[i])\n",
    "            gene_to_idx = {gene: idx for idx, gene in enumerate(genes[i])}\n",
    "            for j, gene in enumerate(union_genes):\n",
    "                if gene in gene_to_idx:\n",
    "                    X_new[:, j] = X_old[:, gene_to_idx[gene]].toarray().flatten()\n",
    "            datasets[i] = csr_matrix(X_new)\n",
    "        ret_genes = np.array(union_genes)\n",
    "    else:\n",
    "        # Only keep genes in common.\n",
    "        ret_genes = np.array(sorted(keep_genes))\n",
    "        for i in range(len(datasets)):\n",
    "            # Remove duplicate genes.\n",
    "            uniq_genes, uniq_idx = np.unique(genes[i], return_index=True)\n",
    "            datasets[i] = datasets[i].tocsc()[:, uniq_idx]\n",
    "            # Do gene filtering.\n",
    "            gene_sort_idx = np.argsort(uniq_genes)\n",
    "            gene_idx = [idx for idx in gene_sort_idx if uniq_genes[idx] in keep_genes]\n",
    "            datasets[i] = datasets[i][:, gene_idx].tocsr()\n",
    "            assert np.array_equal(uniq_genes[gene_idx], ret_genes)\n",
    "\n",
    "    return datasets, ret_genes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create AnnData objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anndata import AnnData\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import scanpy as sc\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrationmapping = {\n",
    "        1: \"10Xmouse\",\n",
    "        2: \"humanpancreas\",\n",
    "        3: \"10Xpbmc\",\n",
    "    }\n",
    "\n",
    "datamapping = {\n",
    "    1: \"pancreas_inDrop\",\n",
    "    2: \"pancreas_multi_celseq2_expression_matrix\",\n",
    "    3: \"pancreas_multi_celseq_expression_matrix\",\n",
    "    4: \"pancreas_multi_fluidigmc1_expression_matrix\",\n",
    "    5: \"pancreas_multi_smartseq2_expression_matrix\",\n",
    "    6: \"jurkat_293t_50_50\",\n",
    "    7: \"jurkat\",\n",
    "    8: \"293t\",\n",
    "    9: \"jurkat_293t_99_1\",\n",
    "    10: \"68k_pbmc\",\n",
    "    11: \"b_cells\",\n",
    "    12: \"cd14_monocytes\",\n",
    "    13: \"cd4_t_helper\",\n",
    "    14: \"cd56_nk\",\n",
    "    15: \"cytotoxic_t\",\n",
    "    16: \"memory_t\",\n",
    "    17: \"regulatory_t\",\n",
    "    18: \"pbmc_10X\",\n",
    "    19: \"pbmc_kang\",\n",
    "}\n",
    "\n",
    "optionmapping = {\n",
    "    1: [6, 7, 8],\n",
    "    2: [1, 2, 3, 4, 5],\n",
    "    3: [11, 12, 13, 14, 15, 16, 17, 18],\n",
    "}\n",
    "\n",
    "def batch_labels(Results,counter):\n",
    "    dataset_labels = []\n",
    "    no_datasets = len (Results['CountMatrix'])\n",
    "    for i in range(0,no_datasets):\n",
    "        datasetsize = Results['CountMatrix'][i].shape[0]\n",
    "        for j in range(0,datasetsize):\n",
    "            dataset_labels.append(datamapping[optionmapping[counter][i]])\n",
    "    return dataset_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cell_labels(counter):\n",
    "\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    \n",
    "    integrationmapping = {\n",
    "        2: \"humanpancreas\",\n",
    "        1: \"10Xmouse\",\n",
    "        3: \"10Xpbmc\",\n",
    "    }\n",
    "    \n",
    "\n",
    "    if counter <= 5:\n",
    "        cell_labels = (\n",
    "            open(\"../Real/scanorama/cell_labels/{}_cell_labels.txt\".format(integrationmapping[counter]))\n",
    "            .read()\n",
    "            .rstrip()\n",
    "            .split()\n",
    "        )\n",
    "\n",
    "    if counter <= 5:\n",
    "        le = LabelEncoder().fit(cell_labels)\n",
    "        new_cell_labels = le.transform(cell_labels)\n",
    "        cell_types = le.classes_\n",
    "        print(\"There are {} types of cells\".format(len(cell_types)))\n",
    "        print(cell_types)\n",
    "\n",
    "    return cell_labels, new_cell_labels, len(cell_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,4):\n",
    "    read_cell_labels(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrationmapping = {\n",
    "        2: \"humanpancreas\",\n",
    "        1: \"10Xmouse\",\n",
    "        3: \"10Xpbmc\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for datasetnumber in range(2,4):\n",
    "    casename = integrationmapping[datasetnumber]\n",
    "\n",
    "    # Count Dataset \n",
    "    genecountdatalist = list()\n",
    "    gene_arraylist = list()\n",
    "    batch_labels = list()\n",
    "    datamapping = { 1:'pancreas_inDrop', 2:'pancreas_multi_celseq2_expression_matrix',\n",
    "                    4:'pancreas_multi_fluidigmc1_expression_matrix', 5:'pancreas_multi_smartseq2_expression_matrix',\n",
    "                    3:'pancreas_multi_celseq_expression_matrix',\n",
    "                    7:'jurkat', 6:'293t', 8:'jurkat_293t_50_50', 9:'jurkat_293t_99_1',\n",
    "                    10:'68k_pbmc', 11:'b_cells', 12:'cd14_monocytes', 13:'cd4_t_helper', 14:'cd56_nk',\n",
    "                    15:'cytotoxic_t', 16:'memory_t', 17:'regulatory_t', 18:'pbmc_10X', 19:'pbmc_kang',\n",
    "                    20:'nuclei', 21:'Cerebellum_ALT', 22:'Cortex_noRep5_FRONTALonly', 23:'Cortex_noRep5_POSTERIORonly',\n",
    "                    24:'EntoPeduncular', 25:'GlobusPallidus', 26:'Hippocampus', 27:'Striatum',\n",
    "                    28:'SubstantiaNigra', 29:'Thalamus',  }\n",
    "    optionmapping = {\n",
    "        1: [6, 7, 8],\n",
    "        2: [1, 2, 3, 4, 5],\n",
    "        3: [11, 12, 13, 14, 15, 16, 17, 18],\n",
    "    }\n",
    "\n",
    "    for datasetnumber in optionmapping[datasetnumber]:\n",
    "        datasetname, genecountdata, gene_array = read_real_data(datasetnumber)\n",
    "        batch_label = [datasetname for x in range(genecountdata.shape[0])]\n",
    "        batch_labels.extend(batch_label)\n",
    "        genecountdatalist.append(genecountdata)\n",
    "        gene_arraylist.append(gene_array)\n",
    "\n",
    "    merged_counts, shared_genes = merge_datasets(genecountdatalist, gene_arraylist)\n",
    "    \n",
    "    # Actual cell types\n",
    "    cell_labels, _, _ = read_cell_labels(datasetnumber)\n",
    "    cell_annotations = pd.DataFrame(cell_labels)\n",
    "    cell_annotations.columns = ['actual']\n",
    "    \n",
    "    # Annotations \n",
    "    gene_annotations = pd.DataFrame(index=shared_genes)\n",
    "    cell_annotations['batch'] = batch_labels\n",
    "\n",
    "    from scipy.sparse import vstack\n",
    "    X = vstack(merged_counts)\n",
    "    # AnnData \n",
    "    mergeddata = AnnData(X,obs=cell_annotations,var=gene_annotations)\n",
    "    filename = '../{}_v2.h5ad'.format(casename)\n",
    "    mergeddata.write(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data to upload to Zenodo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in ['10Xmouse','humanpancreas','10Xpbmc']:\n",
    "    batch = dict()\n",
    "    actual = dict()\n",
    "    adata = sc.read('../Data/{}_v2.h5ad'.format(dataset))\n",
    "    filename = '../Data/Finalised_Data/{}_data_only.h5ad'.format(dataset)\n",
    "    newadata = AnnData(adata.X,obs=adata.obs[['actual','batch']],var=adata.var)\n",
    "    print(newadata)   \n",
    "    newadata.write(filename)\n",
    "    \n",
    "for dataset in ['10Xmouse','humanpancreas','10Xpbmc']:\n",
    "    batch = dict()\n",
    "    actual = dict()\n",
    "    adata = sc.read('../Data/{}_v4_processed.h5ad'.format(dataset))\n",
    "    filename = '../Data/Finalised_Data/{}_results_only.h5ad'.format(dataset)\n",
    "    adata.X = None \n",
    "    adata.write(filename)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from anndata import AnnData\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import scanpy as sc\n",
    "from scipy.stats import entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AnnData object with n_obs × n_vars = 9530 × 32643\n",
      "    obs: 'actual', 'batch', 'IHPF_0.001_kmeans_normalised', 'IHPF_0.001_max', 'IHPF_0.01_kmeans_normalised', 'IHPF_0.01_max', 'IHPF_0.1_kmeans_normalised', 'IHPF_0.1_max', 'IHPF_0.25_kmeans_normalised', 'IHPF_0.25_max', 'IHPF_0.5_kmeans_normalised', 'IHPF_0.5_max', 'IHPF_0.75_kmeans_normalised', 'IHPF_0.75_max', 'HPF_kmeans_normalised', 'HPF_max'\n",
      "    obsm: 'HPF', 'IHPF_0.001', 'IHPF_0.01', 'IHPF_0.1', 'IHPF_0.25', 'IHPF_0.5', 'IHPF_0.75'\n",
      "    varm: 'HPF', 'IHPF_0.001', 'IHPF_0.01', 'IHPF_0.1', 'IHPF_0.25', 'IHPF_0.5', 'IHPF_0.75'\n",
      "AnnData object with n_obs × n_vars = 15921 × 15369\n",
      "    obs: 'actual', 'batch', 'IHPF_0.001_kmeans_normalised', 'IHPF_0.001_max', 'IHPF_0.01_kmeans_normalised', 'IHPF_0.01_max', 'IHPF_0.1_kmeans_normalised', 'IHPF_0.1_max', 'IHPF_0.25_kmeans_normalised', 'IHPF_0.25_max', 'IHPF_0.5_kmeans_normalised', 'IHPF_0.5_max', 'IHPF_0.75_kmeans_normalised', 'IHPF_0.75_max', 'HPF_kmeans_normalised', 'HPF_max'\n",
      "    obsm: 'HPF', 'IHPF_0.001', 'IHPF_0.01', 'IHPF_0.1', 'IHPF_0.25', 'IHPF_0.5', 'IHPF_0.75'\n",
      "    varm: 'HPF', 'IHPF_0.001', 'IHPF_0.01', 'IHPF_0.1', 'IHPF_0.25', 'IHPF_0.5', 'IHPF_0.75'\n",
      "AnnData object with n_obs × n_vars = 26202 × 32643\n",
      "    obs: 'actual', 'batch', 'IHPF_0.001_kmeans_normalised', 'IHPF_0.001_max', 'IHPF_0.01_kmeans_normalised', 'IHPF_0.01_max', 'IHPF_0.1_kmeans_normalised', 'IHPF_0.1_max', 'IHPF_0.25_kmeans_normalised', 'IHPF_0.25_max', 'IHPF_0.5_kmeans_normalised', 'IHPF_0.5_max', 'IHPF_0.75_kmeans_normalised', 'IHPF_0.75_max', 'HPF_kmeans_normalised', 'HPF_max'\n",
      "    obsm: 'HPF', 'IHPF_0.001', 'IHPF_0.01', 'IHPF_0.1', 'IHPF_0.25', 'IHPF_0.5', 'IHPF_0.75'\n",
      "    varm: 'HPF', 'IHPF_0.001', 'IHPF_0.01', 'IHPF_0.1', 'IHPF_0.25', 'IHPF_0.5', 'IHPF_0.75'\n"
     ]
    }
   ],
   "source": [
    "for dataset in ['10Xmouse','humanpancreas','10Xpbmc']:\n",
    "    batch = dict()\n",
    "    actual = dict()\n",
    "    filename = '../Data/Finalised_Data/{}.h5ad'.format(dataset)\n",
    "    adata = sc.read(filename)\n",
    "    print(adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ihpf",
   "language": "python",
   "name": "ihpf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
